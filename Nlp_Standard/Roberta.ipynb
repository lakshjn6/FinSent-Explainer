{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb89b5b-5cd8-4b03-9ec4-0106c7724132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 1: INSTALL AND IMPORT LIBRARIES\n",
    "# ===================================================================\n",
    "# First, install required libraries:\n",
    "# pip install transformers torch pandas numpy\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD THE ROBERTA TOKENIZER\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2: Loading RoBERTa Tokenizer\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the tokenizer - this downloads it first time\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "print(f\"✅ Tokenizer loaded!\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Maximum sequence length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: PREPARE SAMPLE COMPLAINT DATA\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 3: Prepare Sample Data\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Your complaint texts (this would be your complaint column)\n",
    "complaint_texts = [\n",
    "    \"I am very disappointed with the service quality\",\n",
    "    \"The product arrived damaged and I want a refund\",\n",
    "    \"Excellent service, very satisfied with the purchase\",\n",
    "    \"Delivery was late and customer support was rude\",\n",
    "    \"Amazing product, exactly what I ordered\"\n",
    "]\n",
    "\n",
    "# Labels (1 = complaint, 0 = not complaint)\n",
    "labels = [1, 1, 0, 1, 0]\n",
    "\n",
    "print(\"Sample complaint texts:\")\n",
    "for i, text in enumerate(complaint_texts):\n",
    "    label_text = \"Complaint\" if labels[i] == 1 else \"Not Complaint\"\n",
    "    print(f\"{i+1}. [{label_text}] {text}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: SEE HOW TOKENIZER BREAKS DOWN TEXT\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 4: Understanding Tokenization\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Take first complaint text as example\n",
    "sample_text = complaint_texts[0]\n",
    "print(f\"Original text: '{sample_text}'\")\n",
    "\n",
    "# Step 4a: Break text into tokens\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"Broken into tokens: {tokens}\")\n",
    "\n",
    "# Step 4b: Convert tokens to numbers\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Token numbers: {token_ids}\")\n",
    "\n",
    "# Step 4c: Convert back to check\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded back: '{decoded_text}'\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: CONVERT ONE TEXT TO VECTORS (SIMPLE WAY)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 5: Convert Single Text to Vectors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Take one complaint text\n",
    "single_text = \"The service was terrible and I want my money back\"\n",
    "print(f\"Converting text: '{single_text}'\")\n",
    "\n",
    "# Convert text to token IDs with padding and truncation\n",
    "encoded = tokenizer(\n",
    "    single_text,\n",
    "    max_length=50,  # Maximum length (you can change this)\n",
    "    padding='max_length',  # Pad shorter texts\n",
    "    truncation=True,  # Cut longer texts\n",
    "    return_tensors='pt'  # Return as PyTorch tensors\n",
    ")\n",
    "\n",
    "print(f\"Input IDs shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask']}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: LOAD ROBERTA MODEL TO GET ACTUAL VECTORS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 6: Load RoBERTa Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the RoBERTa model\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"✅ RoBERTa model loaded!\")\n",
    "print(f\"Model hidden size: {model.config.hidden_size}\")  # Should be 768\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 7: GET ACTUAL VECTORS FROM TEXT\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 7: Convert Text to Actual Vectors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the encoded text from step 5\n",
    "input_ids = encoded['input_ids']\n",
    "attention_mask = encoded['attention_mask']\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Pass through RoBERTa model\n",
    "with torch.no_grad():  # Don't calculate gradients (saves memory)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "# Get the vectors (embeddings)\n",
    "vectors = outputs.last_hidden_state\n",
    "print(f\"Output vectors shape: {vectors.shape}\")\n",
    "print(f\"Each word is now a {vectors.shape[-1]}-dimensional vector!\")\n",
    "\n",
    "# Show first few dimensions of first word vector\n",
    "print(f\"First word vector (first 10 dimensions): {vectors[0, 0, :10]}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 8: PROCESS MULTIPLE TEXTS (YOUR COMPLAINT COLUMN)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 8: Process Multiple Complaint Texts\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Function to convert multiple texts to vectors\n",
    "def convert_texts_to_vectors(texts, tokenizer, model, max_length=128):\n",
    "    \"\"\"\n",
    "    Simple function to convert list of texts to vectors\n",
    "    \"\"\"\n",
    "    all_vectors = []\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts...\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"Processing text {i+1}: '{text[:50]}...'\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get vectors from model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=encoded['input_ids'],\n",
    "                attention_mask=encoded['attention_mask']\n",
    "            )\n",
    "            vectors = outputs.last_hidden_state\n",
    "        \n",
    "        # Store results\n",
    "        all_vectors.append(vectors)\n",
    "        all_input_ids.append(encoded['input_ids'])\n",
    "        all_attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    # Combine all vectors into one tensor\n",
    "    all_vectors = torch.cat(all_vectors, dim=0)\n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "    \n",
    "    return all_vectors, all_input_ids, all_attention_masks\n",
    "\n",
    "# Convert all complaint texts to vectors\n",
    "vectors, input_ids, attention_masks = convert_texts_to_vectors(\n",
    "    complaint_texts, tokenizer, model, max_length=64\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ All texts converted!\")\n",
    "print(f\"Final vectors shape: {vectors.shape}\")\n",
    "print(f\"This means: {vectors.shape[0]} texts, {vectors.shape[1]} words each, {vectors.shape[2]} dimensions per word\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 9: SAVE AND LOAD YOUR VECTORS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 9: Save Your Vectors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save vectors to file\n",
    "torch.save({\n",
    "    'vectors': vectors,\n",
    "    'input_ids': input_ids,\n",
    "    'attention_masks': attention_masks,\n",
    "    'labels': torch.tensor(labels),\n",
    "    'texts': complaint_texts\n",
    "}, 'complaint_vectors.pt')\n",
    "\n",
    "print(\"✅ Vectors saved to 'complaint_vectors.pt'\")\n",
    "\n",
    "# Load vectors back\n",
    "loaded_data = torch.load('complaint_vectors.pt')\n",
    "print(f\"✅ Loaded back! Shape: {loaded_data['vectors'].shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 10: SIMPLE EXAMPLE WITH YOUR OWN DATA\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 10: Example with Your Own Data\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# This is how you would use it with your own CSV file\n",
    "def process_your_complaint_data(csv_file_path, complaint_column_name):\n",
    "    \"\"\"\n",
    "    Simple function to process your own complaint data\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from: {csv_file_path}\")\n",
    "    \n",
    "    # Read your CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    \n",
    "    # Get complaint texts\n",
    "    complaint_texts = df[complaint_column_name].astype(str).tolist()\n",
    "    print(f\"Found {len(complaint_texts)} complaint texts\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaModel.from_pretrained('roberta-base')\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to vectors\n",
    "    vectors, input_ids, attention_masks = convert_texts_to_vectors(\n",
    "        complaint_texts, tokenizer, model\n",
    "    )\n",
    "    \n",
    "    return vectors, input_ids, attention_masks, complaint_texts\n",
    "\n",
    "# Example usage (uncomment when you have your data):\n",
    "# vectors, input_ids, attention_masks, texts = process_your_complaint_data(\n",
    "#     'your_data.csv', 'complaint_column'\n",
    "# )\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 11: WHAT HAPPENS NEXT (FOR YOUR MODEL)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 11: What Happens Next\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Now you have vectors for your complaints!\")\n",
    "print(f\"Vector shape: {vectors.shape}\")\n",
    "print(\"\\nThese vectors will go to:\")\n",
    "print(\"1. BiGRU layer (processes sequence)\")\n",
    "print(\"2. Attention layer (focuses on important parts)\")\n",
    "print(\"3. CentralNet (makes final prediction)\")\n",
    "print(\"\\nResult: Complaint or Not Complaint prediction!\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 12: QUICK TEST - PREDICT NEW COMPLAINT\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 12: Test with New Complaint\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test with a new complaint\n",
    "new_complaint = \"This product is broken and customer service won't help me\"\n",
    "print(f\"New complaint: '{new_complaint}'\")\n",
    "\n",
    "# Convert to vectors\n",
    "encoded = tokenizer(\n",
    "    new_complaint,\n",
    "    max_length=64,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=encoded['input_ids'],\n",
    "        attention_mask=encoded['attention_mask']\n",
    "    )\n",
    "    new_vectors = outputs.last_hidden_state\n",
    "\n",
    "print(f\"New complaint vector shape: {new_vectors.shape}\")\n",
    "print(\"✅ Ready to be processed by your BiGRU + Attention + CentralNet model!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: YOUR COMPLAINT TEXTS ARE NOW VECTORS!\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. ✅ Loaded RoBERTa tokenizer and model\")\n",
    "print(\"2. ✅ Converted complaint texts to token IDs\") \n",
    "print(\"3. ✅ Generated 768-dimensional vectors for each word\")\n",
    "print(\"4. ✅ These vectors capture the meaning of your complaints\")\n",
    "print(\"5. ✅ Ready for your BiGRU + Attention + CentralNet model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
