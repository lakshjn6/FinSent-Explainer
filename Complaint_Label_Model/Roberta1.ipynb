{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ef9be6-bcab-4d7c-a23b-61b65dc44b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f24f43-2fc1-49c7-bc1d-ed21e8f4ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31837e46-fde7-40e9-8d11-f14fb791aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer loaded!\n",
      "Vocabulary size: 50265\n",
      "Maximum sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Tokenizer loaded!\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Maximum sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88a78d2-276a-4d6c-9676-98913cf876ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample complaint texts:\n",
      "1. [Complaint] I am very disappointed with the service quality\n",
      "2. [Complaint] The product arrived damaged and I want a refund\n",
      "3. [Not Complaint] Excellent service, very satisfied with the purchase\n",
      "4. [Complaint] Delivery was late and customer support was rude\n",
      "5. [Not Complaint] Amazing product, exactly what I ordered\n"
     ]
    }
   ],
   "source": [
    "complaint_texts = [\n",
    "    \"I am very disappointed with the service quality\",\n",
    "    \"The product arrived damaged and I want a refund\",\n",
    "    \"Excellent service, very satisfied with the purchase\",\n",
    "    \"Delivery was late and customer support was rude\",\n",
    "    \"Amazing product, exactly what I ordered\"\n",
    "]\n",
    "labels = [1, 1, 0, 1, 0]\n",
    "\n",
    "print(\"Sample complaint texts:\")\n",
    "for i, text in enumerate(complaint_texts):\n",
    "    label_text = \"Complaint\" if labels[i] == 1 else \"Not Complaint\"\n",
    "    print(f\"{i+1}. [{label_text}] {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed456ebf-393a-424d-81d6-24365a4b485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 4: Understanding Tokenization\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 4: Understanding Tokenization\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b94ac0-a7f9-48ce-81f9-945e48da44db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 'I am very disappointed with the service quality'\n"
     ]
    }
   ],
   "source": [
    "# Take first complaint text as example\n",
    "sample_text = complaint_texts[0]\n",
    "print(f\"Original text: '{sample_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21887f6-5693-47de-8546-81fece2c0555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broken into tokens: ['I', 'Ġam', 'Ġvery', 'Ġdisappointed', 'Ġwith', 'Ġthe', 'Ġservice', 'Ġquality']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"Broken into tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23acc94a-dc5c-401f-ac1e-6eff53844cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token numbers: [100, 524, 182, 5779, 19, 5, 544, 1318]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Token numbers: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c225793-4816-4bf1-bd6a-2ec2fba947b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded back: 'I am very disappointed with the service quality'\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded back: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd505e7-22a2-4a8a-ad6f-43f8c76088e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 5: Convert Single Text to Vectors\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 5: Convert Single Text to Vectors\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b811e9-07dd-4437-9a37-33097b17ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text: 'The service was terrible and I want my money back'\n"
     ]
    }
   ],
   "source": [
    "single_text = \"The service was terrible and I want my money back\"\n",
    "print(f\"Converting text: '{single_text}'\")\n",
    "\n",
    "# Convert text to token IDs with padding and truncation\n",
    "encoded = tokenizer(\n",
    "    single_text,\n",
    "    max_length=50,  # Maximum length (you can change this)\n",
    "    padding='max_length',  # Pad shorter texts\n",
    "    truncation=True,  # Cut longer texts\n",
    "    return_tensors='pt'  # Return as PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d54a7a94-7db6-4966-8608-524733e90dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 50])\n",
      "Input IDs: tensor([[   0,  133,  544,   21, 6587,    8,   38,  236,  127,  418,  124,    2,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "670dd240-c578-4fba-a308-61c28a81319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 6: Load RoBERTa Model\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 6: LOAD ROBERTA MODEL TO GET ACTUAL VECTORS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 6: Load RoBERTa Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the RoBERTa model\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3c69564-8615-46e5-a27a-6f2dc1111dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RoBERTa model loaded!\n",
      "Model hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ RoBERTa model loaded!\")\n",
    "print(f\"Model hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce45aa71-77a5-4aed-9107-f9c11f63df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 7: GET ACTUAL VECTORS FROM TEXT\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "780ef16a-98b8-40ae-bf64-df9ea860093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 7: Convert Text to Actual Vectors\n",
      "==================================================\n",
      "Input shape: torch.Size([1, 50])\n",
      "Output vectors shape: torch.Size([1, 50, 768])\n",
      "Each word is now a 768-dimensional vector!\n",
      "First word vector (first 10 dimensions): tensor([-0.0439,  0.1084, -0.0172, -0.1317,  0.0851, -0.0702, -0.0444,  0.0221,\n",
      "         0.0571, -0.0701])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 7: Convert Text to Actual Vectors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the encoded text from step 5\n",
    "input_ids = encoded['input_ids']\n",
    "attention_mask = encoded['attention_mask']\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Pass through RoBERTa model\n",
    "with torch.no_grad():  # Don't calculate gradients (saves memory)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "# Get the vectors (embeddings)\n",
    "vectors = outputs.last_hidden_state\n",
    "print(f\"Output vectors shape: {vectors.shape}\")\n",
    "print(f\"Each word is now a {vectors.shape[-1]}-dimensional vector!\")\n",
    "\n",
    "# Show first few dimensions of first word vector\n",
    "print(f\"First word vector (first 10 dimensions): {vectors[0, 0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "504e04ab-3ec9-4bbc-bebf-174c565d641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 texts...\n",
      "Processing text 1: 'I am very disappointed with the service quality...'\n",
      "Processing text 2: 'The product arrived damaged and I want a refund...'\n",
      "Processing text 3: 'Excellent service, very satisfied with the purchas...'\n",
      "Processing text 4: 'Delivery was late and customer support was rude...'\n",
      "Processing text 5: 'Amazing product, exactly what I ordered...'\n",
      "\n",
      "✅ All texts converted!\n",
      "Final vectors shape: torch.Size([5, 64, 768])\n",
      "This means: 5 texts, 64 words each, 768 dimensions per word\n"
     ]
    }
   ],
   "source": [
    "def convert_texts_to_vectors(texts, tokenizer, model, max_length=128):\n",
    "    \"\"\"\n",
    "    Simple function to convert list of texts to vectors\n",
    "    \"\"\"\n",
    "    all_vectors = []\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts...\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"Processing text {i+1}: '{text[:50]}...'\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get vectors from model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=encoded['input_ids'],\n",
    "                attention_mask=encoded['attention_mask']\n",
    "            )\n",
    "            vectors = outputs.last_hidden_state\n",
    "        # Store results\n",
    "        all_vectors.append(vectors)\n",
    "        all_input_ids.append(encoded['input_ids'])\n",
    "        all_attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    # Combine all vectors into one tensor\n",
    "    all_vectors = torch.cat(all_vectors, dim=0)\n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "    \n",
    "    return all_vectors, all_input_ids, all_attention_masks\n",
    "\n",
    "# Convert all complaint texts to vectors\n",
    "vectors, input_ids, attention_masks = convert_texts_to_vectors(\n",
    "    complaint_texts, tokenizer, model, max_length=64\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ All texts converted!\")\n",
    "print(f\"Final vectors shape: {vectors.shape}\")\n",
    "print(f\"This means: {vectors.shape[0]} texts, {vectors.shape[1]} words each, {vectors.shape[2]} dimensions per word\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c3a40e5-f0d9-4a64-83f1-3ba9a4275821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0621,  0.1021, -0.0250,  ..., -0.1021, -0.0638, -0.0200],\n",
       "         [-0.1524,  0.2207, -0.1002,  ...,  0.0227, -0.1117, -0.2606],\n",
       "         [ 0.1015,  0.1897,  0.0340,  ..., -0.1063, -0.1235, -0.0184],\n",
       "         ...,\n",
       "         [-0.0180,  0.0679, -0.0221,  ..., -0.0988, -0.0824,  0.0349],\n",
       "         [-0.0180,  0.0679, -0.0221,  ..., -0.0988, -0.0824,  0.0349],\n",
       "         [-0.0180,  0.0679, -0.0221,  ..., -0.0988, -0.0824,  0.0349]],\n",
       "\n",
       "        [[-0.0525,  0.1079, -0.0360,  ..., -0.0786, -0.0604, -0.0311],\n",
       "         [-0.2472,  0.1690, -0.0486,  ..., -0.1750,  0.1431, -0.2118],\n",
       "         [-0.0501,  0.0207,  0.1038,  ..., -0.1472,  0.0750, -0.0438],\n",
       "         ...,\n",
       "         [ 0.0008,  0.1428, -0.0237,  ..., -0.0710, -0.0644,  0.0336],\n",
       "         [ 0.0008,  0.1428, -0.0237,  ..., -0.0710, -0.0644,  0.0336],\n",
       "         [ 0.0008,  0.1428, -0.0237,  ..., -0.0710, -0.0644,  0.0336]],\n",
       "\n",
       "        [[-0.0504,  0.1014, -0.0167,  ..., -0.1163, -0.0670, -0.0075],\n",
       "         [-0.0319,  0.5034,  0.2318,  ...,  0.1201,  0.2287, -0.0008],\n",
       "         [-0.1245,  0.3145,  0.1598,  ...,  0.1936, -0.0174, -0.0623],\n",
       "         ...,\n",
       "         [-0.0514,  0.1342,  0.0278,  ..., -0.0937, -0.0527,  0.0874],\n",
       "         [-0.0514,  0.1342,  0.0278,  ..., -0.0937, -0.0527,  0.0874],\n",
       "         [-0.0514,  0.1342,  0.0278,  ..., -0.0937, -0.0527,  0.0874]],\n",
       "\n",
       "        [[-0.0438,  0.0791, -0.0037,  ..., -0.0884, -0.0391,  0.0087],\n",
       "         [-0.1106,  0.0990,  0.0373,  ...,  0.2433,  0.1705, -0.0791],\n",
       "         [ 0.0963,  0.0378,  0.1860,  ..., -0.3314,  0.0584, -0.0656],\n",
       "         ...,\n",
       "         [-0.0209,  0.1069,  0.0039,  ..., -0.3994, -0.0327, -0.0126],\n",
       "         [-0.0209,  0.1069,  0.0039,  ..., -0.3994, -0.0327, -0.0126],\n",
       "         [-0.0209,  0.1069,  0.0039,  ..., -0.3994, -0.0327, -0.0126]],\n",
       "\n",
       "        [[-0.0503,  0.0858, -0.0255,  ..., -0.0730, -0.0595, -0.0242],\n",
       "         [-0.0776,  0.1416,  0.0305,  ..., -0.1517,  0.1776,  0.1256],\n",
       "         [-0.0969,  0.1260,  0.1042,  ..., -0.0551,  0.0202,  0.0136],\n",
       "         ...,\n",
       "         [-0.0477,  0.0128, -0.0077,  ..., -0.1989, -0.0873,  0.0791],\n",
       "         [-0.0477,  0.0128, -0.0077,  ..., -0.1989, -0.0873,  0.0791],\n",
       "         [-0.0477,  0.0128, -0.0077,  ..., -0.1989, -0.0873,  0.0791]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d177d1af-4672-4083-b8b0-ee363e2b024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 9: Save Your Vectors\n",
      "==================================================\n",
      "✅ Vectors saved to 'complaint_vectors.pt'\n",
      "✅ Loaded back! Shape: torch.Size([5, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 9: Save Your Vectors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save vectors to file\n",
    "torch.save({\n",
    "    'vectors': vectors,\n",
    "    'input_ids': input_ids,\n",
    "    'attention_masks': attention_masks,\n",
    "    'labels': torch.tensor(labels),\n",
    "    'texts': complaint_texts\n",
    "}, 'complaint_vectors.pt')\n",
    "\n",
    "print(\"✅ Vectors saved to 'complaint_vectors.pt'\")\n",
    "\n",
    "# Load vectors back\n",
    "loaded_data = torch.load('complaint_vectors.pt')\n",
    "print(f\"✅ Loaded back! Shape: {loaded_data['vectors'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0cf391d-72d8-4c82-994f-d3e98651439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 10: Example with Your Own Data\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 10: Example with Your Own Data\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# This is how you would use it with your own CSV file\n",
    "def process_your_complaint_data(csv_file_path, complaint_column_name):\n",
    "    \"\"\"\n",
    "    Simple function to process your own complaint data\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from: {csv_file_path}\")\n",
    "    \n",
    "    # Read your CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    \n",
    "    # Get complaint texts\n",
    "    complaint_texts = df[complaint_column_name].astype(str).tolist()\n",
    "    print(f\"Found {len(complaint_texts)} complaint texts\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaModel.from_pretrained('roberta-base')\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to vectors\n",
    "    vectors, input_ids, attention_masks = convert_texts_to_vectors(\n",
    "        complaint_texts, tokenizer, model\n",
    "    )\n",
    "    \n",
    "    return vectors, input_ids, attention_masks, complaint_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9582ee-e4ed-455a-9500-b4c78fcd4677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d6f29-2d09-4be5-905b-7954a2e5622c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
